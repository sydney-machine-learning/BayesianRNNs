# -*- coding: utf-8 -*-
"""time series prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OHur1xKtAT6gQV065m-ttweVDaCfpAlf
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
import os
import math

# from google.colab import drive
# drive.mount('/content/drive')

training_set = pd.read_csv('./data/Sunspot/train1.csv',index_col=0)
training_set = training_set.values
test_set = pd.read_csv('./data/Sunspot/test1.csv',index_col=0)
test_set = test_set.values


# sc = MinMaxScaler()
# training_set = sc.fit_transform(training_set)
# test_set = sc.fit_transform(test_set)

# dataset = sc.fit_transform(np.concatenate((training_set,test_set)))

n_steps_in, n_steps_out = 5,10

train_x = np.array(training_set[:,0:n_steps_in])
train_y = np.array(training_set[:,n_steps_in:n_steps_in + n_steps_out])

test_x = np.array(test_set[:,0:n_steps_in])
test_y = np.array(test_set[:,n_steps_in: n_steps_in+ n_steps_out])

print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)


trainX = Variable(torch.Tensor(np.array(train_x[:,:,np.newaxis])))
# trainY = Variable(torch.Tensor(np.array(train_y[:,:,np.newaxis])))
# trainX = Variable(torch.Tensor(np.array(train_x)))
trainY = Variable(torch.Tensor(np.array(train_y)))
testX = Variable(torch.Tensor(np.array(test_x[:,:,np.newaxis])))
# testY = Variable(torch.Tensor(np.array(test_y[:,:,np.newaxis])))
# testX = Variable(torch.Tensor(np.array(test_x)))
testY = Variable(torch.Tensor(np.array(test_y)))

print(trainX.shape)
print(trainY.shape)
print(testX.shape)
print(testY.shape)


#input shape is of form(in case of batch first): [num samples, sequence length, input_size]
#output shape is of form(in case of batch first): [num samples, output sequence length, hidden size]

class LSTM(nn.Module):

    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTM, self).__init__()
        
        self.num_classes = num_classes #num_classes is same as number of outcomes(here we have 10 step outcome)
        self.num_layers = num_layers #single layer in case of lstm
        self.input_size = input_size #5
        self.hidden_size = hidden_size #10
        # self.seq_length = seq_length
        
        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)
        
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
        # print(h_out.shape)
        h_out = h_out.view(-1, self.hidden_size)
        # print(h_out.shape)
        out = self.fc(h_out)
        
        return out

def rmse( pred, actual):
        if len(actual.shape)>2 and actual.shape[2]==1: actual = np.squeeze(actual,axis = 2)
        if len(pred.shape) > 2 and actual.shape[2]==1: pred = np.squeeze(pred, axis = 2)
        step_wise_rmse = np.sqrt(np.mean((pred - actual)**2, axis = 0))
        # old_rmse = np.sqrt(((pred-actual)**2).mean())
        # print(f"old rmse in code: {old_rmse}")
        new_rmse = np.mean(step_wise_rmse)
        # print(f'rmse : {new_rmse}')
        return new_rmse

def step_wise_rmse( pred, actual):
        if len(actual.shape)>2 and actual.shape[2]==1: actual = np.squeeze(actual,axis = 2)
        if len(pred.shape) > 2 and actual.shape[2]==1: pred = np.squeeze(pred, axis = 2)
        ans = np.sqrt(np.mean((pred - actual)**2, axis = 0))
        np.mean()
        # print("shape of step_wise_rmse: ",ans.shape)
        return ans

def likelihood_func(fx,y, tau_sq =0.01, temp = 1):
 
        # fx = rnn.evaluate_proposal(x,w)
 
        # rmse_val = rmse(fx, y)
        # step_wise_rmse_val = self.step_wise_rmse(fx,y) 

        n = y.shape[0] * y.shape[1]

        p1 = -(n/2)*np.log(2*math.pi*tau_sq) 

        p2 = (1/(2*tau_sq)) 

        log_lhood = p1 -  (p2 * np.sum(np.square(y -fx)) )

        pseudo_loglhood =  log_lhood/temp       
        #question: what should go into parameter queue? log_lhood or pseudo_loglhood
        return pseudo_loglhood

num_epochs = 2000
learning_rate = 0.05

input_size = 1
hidden_size = 10
num_layers = 1

n_steps_ahead = 10

lstm = LSTM( input_size, hidden_size, num_layers, n_steps_ahead)

criterion = torch.nn.MSELoss()    # mean-squared error for regression
optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)
#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)


lhood = np.zeros(shape=(num_epochs,1))

# Train the model
for epoch in range(num_epochs):
    outputs = lstm(trainX)
    # print(outputs.shape, trainY.shape)
    lhood_val = likelihood_func(outputs.detach().numpy(), trainY.detach().numpy())
    lhood[epoch,0]=lhood_val
    optimizer.zero_grad()
    
    # obtain the loss function
    loss = criterion(outputs, trainY)
    
    loss.backward()
    
    optimizer.step()
    if epoch % 5 == 0:
      print("Epoch: %d, loss: %1.5f" % (epoch, loss.item()))

lstm.eval()
dataX = Variable(torch.Tensor(np.concatenate((trainX,testX))))
dataY = Variable(torch.Tensor(np.concatenate((trainY, testY))))
train_predict = lstm(dataX)
#shape (970,10)

data_predict = train_predict.data.numpy()
print(data_predict.shape)
dataY_plot = dataY.data.numpy()

# data_predict = sc.inverse_transform(data_predict)
# dataY_plot = sc.inverse_transform(dataY_plot)

train_size = trainX.shape[0]
test_size = testX.shape[0]
plt.figure(figsize=(10,5))
plt.axvline(x=train_size, c='r', linestyle= '--')

plt.plot(dataY_plot[:,0])
plt.plot(data_predict[:,0])
plt.suptitle('Time-Series Prediction for Univariate case but showing results for 1st step only:')
plt.show()

plt.figure(figsize = (10,5))
plt.plot(lhood)

"""MCMC"""



