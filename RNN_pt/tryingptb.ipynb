{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "#np.random.seed(1)\n",
    "import nn_mcmc_plots as mcmcplt\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import time\n",
    "#np.random.seed(1)\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import gc\n",
    "import matplotlib as mpl\n",
    "mpl.use('agg')\n",
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout, TimeDistributed, Reshape, Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "mplt = mcmcplt.Mcmcplot()\n",
    "weightdecay = 0.01\n",
    "\n",
    "def f(): raise Exception(\"Found exit()\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = 'data/ptb.train.txt'\n",
    "    valid_path = 'data/ptb.valid.txt'\n",
    "    test_path = 'data/ptb.test.txt'\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(vocabulary,' - length of vocabulary')\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary\n",
    "\n",
    "\n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        flag = 0\n",
    "        while True:\n",
    "            if(flag == 1):\n",
    "                break\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                    flag = 1\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000  - length of vocabulary\n1860  is the length of train dataset\n165  is the length of test dataset\nshape of input sample is (x,y)  (20, 25) (20, 25, 10000)\nnumber of samples is: for train and test -  1000 165\n"
    }
   ],
   "source": [
    "name = \"ptb\"\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n",
    "num_steps = 25\n",
    "batch_size = 20\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,skip_step=num_steps)\n",
    "test_data_generator = KerasBatchGenerator(test_data, num_steps, batch_size, vocabulary,skip_step=num_steps)\n",
    "trainData = list(train_data_generator.generate())\n",
    "testData = list(test_data_generator.generate())\n",
    "print(len(trainData), ' is the length of train dataset')\n",
    "print(len(testData), ' is the length of test dataset')\n",
    "\n",
    "'''x,y = trainData[0]\n",
    "print(x.shape,y.shape)'''\n",
    "train_x=[]\n",
    "train_y=[]\n",
    "test_x = []\n",
    "test_y = []\n",
    "idx = 0\n",
    "breakat = 1000\n",
    "for iter1 in trainData:\n",
    "    if(idx == breakat):\n",
    "        break\n",
    "    idx+=1\n",
    "    a,b = iter1\n",
    "    train_x.append(a.astype(int))\n",
    "    train_y.append(b)\n",
    "\n",
    "for iter2 in testData:\n",
    "    c,d = iter2\n",
    "    test_x.append(c.astype(int))\n",
    "    test_y.append(d)\n",
    "\n",
    "# shape of inputs\n",
    "print('shape of input sample is (x,y) ',train_x[0].shape,train_y[0].shape)\n",
    "print('number of samples is: for train and test - ', len(train_x), len(test_x))\n",
    "# trimming x and y\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.IntTensor(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "        # Defining input size, hidden layer size, output size and batch size respectively\n",
    "    def __init__(self, topo,lrate,rnn_net = 'LSTM'):\n",
    "        super(Model, self).__init__()\n",
    "        # assuming num_dicrections to be 1 for all the cases\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = topo[1] #hidden_dim\n",
    "        self.n_layers = 1\n",
    "        self.batch_size = batch_size\n",
    "        self.lrate = lrate\n",
    "        if rnn_net == 'RNN':\n",
    "            self.hidden = torch.ones(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "            self.rnn = nn.RNN(input_size = topo[0], hidden_size = topo[1])\n",
    "\n",
    "        if rnn_net == 'GRU':\n",
    "            self.hidden = torch.ones((self.n_layers,self.batch_size,self.hidden_dim))\n",
    "            self.rnn = nn.GRU(input_size = topo[0], hidden_size = topo[1])\n",
    "\n",
    "        if rnn_net == 'LSTM':\n",
    "            self.hidden = (torch.ones((self.n_layers,self.batch_size,self.hidden_dim)), torch.ones((self.n_layers,self.batch_size,self.hidden_dim)))\n",
    "            self.rnn = nn.LSTM(input_size = topo[0], hidden_size = topo[1])\n",
    "\n",
    "            # Fully connected layer\n",
    "        self.fc = nn.Linear(topo[1],topo[2])\n",
    "        self.topo = topo\n",
    "        self.vocab_size = vocabulary\n",
    "        self.embed = nn.Embedding(self.vocab_size,topo[0])\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        print(rnn_net, ' is rnn net')\n",
    "\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+torch.exp(-z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outmain=torch.zeros((len(x),self.batch_size,num_steps,vocabulary))\n",
    "        for i,sample in enumerate(x):\n",
    "            print(i,'th sample running')\n",
    "            # sample = torch.FloatTensor(sample)\n",
    "            sample = torch.LongTensor(sample)\n",
    "            # print(sample)\n",
    "            # input of shape (seq_len, batch, input_size):\n",
    "            # print('shape 1',sample.shape)\n",
    "            sample = self.embed(sample)\n",
    "            sample = sample.reshape(num_steps,batch_size,self.topo[0])\n",
    "            # print('shape 2',sample.shape)\n",
    "            # sample = sample.view(sample.shape[0],1,self.topo[0])\n",
    "            hidden = copy.deepcopy(self.hidden)\n",
    "            out, h1 = self.rnn(sample, hidden)\n",
    "            out = self.fc(out)\n",
    "            # out = self.sigmoid(out)\n",
    "            # print(' shape of out',out.shape)\n",
    "            out = self.softmax(out) # check dimension right or not\n",
    "            outmain[i,] = copy.deepcopy(out.detach()).reshape(self.batch_size,num_steps,vocabulary)\n",
    "        return outmain\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden        # Create a brnn -- assuming h0 size = 5 i.e. hidden layer has 5 nuerons\n",
    "\n",
    "    def dictfromlist(self,param):\n",
    "        dic = {}\n",
    "        i=0\n",
    "        for name in sorted(self.state_dict().keys()):\n",
    "            dic[name] = torch.FloatTensor(param[i:i+(self.state_dict()[name]).view(-1).shape[0]]).view(self.state_dict()[name].shape)\n",
    "            i += (self.state_dict()[name]).view(-1).shape[0]\n",
    "        #self.loadparameters(dic)\n",
    "        return dic\n",
    "        \n",
    "    def evaluate_proposal(self,x,w=None):\n",
    "        if w is None:\n",
    "            y_pred = self.forward(x)\n",
    "            return y_pred.detach().numpy()\n",
    "        else:\n",
    "            self.loadparameters(w)\n",
    "            y_pred = self.forward(x)\n",
    "            return np.array(y_pred.detach())\n",
    "\n",
    "\n",
    "#    def loss(self,fx,y):\n",
    "#        fx = torch.FloatTensor(fx)\n",
    "#        y = torch.FloatTensor(y)\n",
    "#        criterion = nn.MSELoss()\n",
    "#        loss = criterion(fx,y)\n",
    "#        return loss.item()\n",
    "\n",
    "    def langevin_gradient(self,x,y,w):\n",
    "        #print(w)\n",
    "        self.loadparameters(w)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters(),lr = self.lrate)\n",
    "        for i,sample in enumerate(x):\n",
    "            # sample = torch.FloatTensor(sample).view(sample.shape[0],1,self.topo[0])\n",
    "            hidden = copy.deepcopy(self.hidden)\n",
    "            optimizer.zero_grad()\n",
    "            sample = self.embed(sample)\n",
    "            out, h1 = self.rnn(sample, hidden)\n",
    "            out = self.fc(out[-1])\n",
    "            out = self.sigmoid(out)\n",
    "            loss = criterion(out,torch.FloatTensor(y[i]).view(out.shape))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return copy.deepcopy(self.state_dict())\n",
    "\n",
    "\n",
    "    #returns a np arraylist of weights and biases -- layerwise\n",
    "    # in order i.e. weight and bias of input and hidden then weight and bias for hidden to out\n",
    "    def getparameters(self,w = None):\n",
    "        l=np.array([1,2])\n",
    "        dic = {}\n",
    "        if w is None:\n",
    "            dic = self.state_dict()\n",
    "        else:\n",
    "            dic = copy.deepcopy(w)\n",
    "        for name in sorted(dic.keys()):\n",
    "            l=np.concatenate((l,np.array(copy.deepcopy(dic[name])).reshape(-1)),axis=None)\n",
    "        l = l[2:]\n",
    "        return l\n",
    "\n",
    "\n",
    "    # input a dictionary of same dimensions\n",
    "    def loadparameters(self,param):\n",
    "        self.load_state_dict(param)\n",
    "\n",
    "    # input weight dictionary, mean, std dev\n",
    "    def addnoiseandcopy(self,w,mea,std_dev):\n",
    "        dic = {}\n",
    "        for name in (w.keys()):\n",
    "            dic[name] = copy.deepcopy(w[name]) + torch.zeros(w[name].size()).normal_(mean = mea, std = std_dev)\n",
    "        return dic\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "LSTM  is rnn net\nyaha tak to done hai\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!18\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-ed1b69bb5f9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yaha tak to done hai\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mpred_train\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_proposal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' reached 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mpred_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_proposal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-f314c02aabab>\u001b[0m in \u001b[0;36mevaluate_proposal\u001b[1;34m(self, x, w)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-f314c02aabab>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0moutmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'th sample running'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!18\n"
     ]
    }
   ],
   "source": [
    "Input = 60\n",
    "Output = vocabulary\n",
    "Hidden = 100\n",
    "#ip = 4 #input\n",
    "#output = 1\n",
    "topology = [Input, Hidden, Output,vocabulary]\n",
    "\n",
    "\n",
    "learn_rate = 0.01\n",
    "rnn = Model(topology,learn_rate)\n",
    "w = copy.deepcopy(rnn.state_dict())\n",
    "step_w = 0.025\n",
    "w_size = sum(p.numel() for p in rnn.parameters())\n",
    "print(\"yaha tak to done hai\")\n",
    "for i in range(1):\n",
    "    pred_train  = rnn.evaluate_proposal(train_x,w) #\n",
    "    print(' reached 1')\n",
    "    pred_test  = rnn.evaluate_proposal(test_x, w) #\n",
    "    print('reached 2')\n",
    "    w_gd = rnn.langevin_gradient(train_x,train_y, copy.deepcopy(w)) # Eq 8\n",
    "    w_proposal = rnn.addnoiseandcopy(w_gd,0,step_w) #np.random.normal(w_gd, step_w, w_size) # Eq 7\n",
    "    w_prop_gd = rnn.langevin_gradient(train_x,train_y, copy.deepcopy(w_proposal))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.5837,  0.5330, -1.9849],\n        [ 0.6244,  0.9341, -0.2163]])\ntensor([[0.4933, 0.4689, 0.0378],\n        [0.3579, 0.4878, 0.1544]])\n"
    }
   ],
   "source": [
    "test_y[0].shape\n",
    "m = nn.Softmax(dim=1)\n",
    "inputa = torch.randn(2, 3)\n",
    "output = m(inputa)\n",
    "print(inputa)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(10000,3)\n",
    "# # x = torch.tensor([3])\n",
    "# # x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]])\n",
    "# x = torch.LongTensor(np.transpose(train_x[0]))\n",
    "# print(x.shape)\n",
    "# embedded = embedding(x)\n",
    "# print(embedded)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}